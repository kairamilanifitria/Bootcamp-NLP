{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kairamilanifitria/Bootcamp-NLP/blob/main/Advanced/04102024_Train_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_8y28uDDlRIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8630f49b-32d7-4b4b-d1de-7582d2144b4d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 10 \"/content/drive/MyDrive/Bootcamp AI/Dataset/clean_id.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ9daJ-3lS-O",
        "outputId": "35a14df6-aea2-4629-d90d-30b0b38876c6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "”Kening Aie yang terangkat naik makin tinggi.\n",
            "Bagaimana aktor-aktris senior dan yang punya bakat akting luarbiasa bagus itu bisa diyakinkan untuk tampil superserius atas skenario yang sesungguhnya tak serius?\n",
            "Nah, ingin tahu lebih detil tentang stadium dan aneka ragam istilah perawatan medis yang harus dilalui oleh penderita kanker payudara?\n",
            "Wiro hanya bisa menyengir dan geleng-gelengkan kepala.\n",
            "15:19) Ibadah yang berisikan berbagai bentuk ritual merupakan hal yang sia-sia apabila tidak disertai oleh hati yang tulus, bersih, dan jujur.\n",
            "Jalanan yang lancar ini kuharap adalah pertanda bahwa niatanku mulia.\n",
            "“saya di tuduh Gerwani” katanya kepada Komparasi beberapa waktu yang lalu.\n",
            "Suatu kelembagaan adalah suatu pemantapan perilaku (ways) yang hidup pada suatu kelompok orang.\n",
            "Belum ada penelitian yang mendalam dan menghasilkan rekomendasi kebijakan,” papar Djati.\n",
            "Tapi tentu saja harpa itu tetap ada di situ meski pikirannya sudah kembali sejernih air.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch==2.1.0\n",
        "! pip install tokenizers==2.4.1\n",
        "! pip install transformers==2.11.0"
      ],
      "metadata": {
        "id": "uOktJcSFnXz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff738284-b3a1-4e47-871c-d635c38436b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.1.0\n",
            "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0)\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0) (12.6.68)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
            "Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.3.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.3.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.3.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.6.59\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.6.59:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.6.59\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.68\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.68:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.68\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.68\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.68:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.68\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.1.4\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.1.4:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.1.4\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.4.69\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.4.69:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.4.69\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.4.0.58\n",
            "    Uninstalling nvidia-cudnn-cu12-9.4.0.58:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.4.0.58\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.1+cu121\n",
            "    Uninstalling torch-2.4.1+cu121:\n",
            "      Successfully uninstalled torch-2.4.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.1.0 which is incompatible.\n",
            "torchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 torch-2.1.0 triton-2.1.0\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 0.12.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tokenizers==2.4.1 (from versions: 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.9, 0.0.10, 0.0.11, 0.0.12, 0.0.13, 0.1.0, 0.1.1, 0.2.0, 0.2.1, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.5.0, 0.5.1, 0.5.2, 0.6.0, 0.7.0rc1, 0.7.0rc2, 0.7.0rc3, 0.7.0rc4, 0.7.0rc5, 0.7.0rc6, 0.7.0rc7, 0.7.0, 0.8.0.dev0, 0.8.0.dev1, 0.8.0.dev2, 0.8.0rc1, 0.8.0rc2, 0.8.0rc3, 0.8.0rc4, 0.8.0, 0.8.1rc1, 0.8.1rc2, 0.8.1, 0.9.0.dev0, 0.9.0.dev1, 0.9.0.dev2, 0.9.0.dev3, 0.9.0.dev4, 0.9.0rc1, 0.9.0rc2, 0.9.0, 0.9.1, 0.9.2, 0.9.3, 0.9.4, 0.10.0rc1, 0.10.0, 0.10.1rc1, 0.10.1, 0.10.2, 0.10.3, 0.11.0, 0.11.1, 0.11.2, 0.11.3, 0.11.4, 0.11.5, 0.11.6, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3rc1, 0.13.3, 0.13.4rc1, 0.13.4rc2, 0.13.4rc3, 0.14.0rc1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2rc1, 0.15.2, 0.19.0, 0.19.1, 0.20.0rc1, 0.20.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tokenizers==2.4.1\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting transformers==2.11.0\n",
            "  Downloading transformers-2.11.0-py3-none-any.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (1.26.4)\n",
            "Collecting tokenizers==0.7.0 (from transformers==2.11.0)\n",
            "  Downloading tokenizers-0.7.0.tar.gz (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (4.66.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (2024.9.11)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (0.2.0)\n",
            "Collecting sacremoses (from transformers==2.11.0)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.11.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.11.0) (1.4.2)\n",
            "Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m674.8/674.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tokenizers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_LAUNCH_BLOCKING=1"
      ],
      "metadata": {
        "id": "6pVqyLzSrLK1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tokenizers"
      ],
      "metadata": {
        "id": "nJwVZOTEouWh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bwpt = tokenizers.BertWordPieceTokenizer()\n",
        "\n",
        "paths = [\"/content/drive/MyDrive/Bootcamp AI/Dataset/clean_id.txt\"]\n",
        "\n",
        "bwpt.train(\n",
        "    files=paths,\n",
        "    vocab_size=50000,\n",
        "    min_frequency=3,\n",
        "    limit_alphabet=1000\n",
        ")\n",
        "\n",
        "bwpt.save_model('.', 'daily-id')"
      ],
      "metadata": {
        "id": "zbbtGj3ho3Jq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5b3f68-1a73-40fe-e226-2867e2b7cb73"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./daily-id-vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, LineByLineTextDataset"
      ],
      "metadata": {
        "id": "jDLo1OgzpA7k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_file_dir = \"./daily-id-vocab.txt\"\n",
        "tokenizer = BertTokenizer.from_pretrained(vocab_file_dir)"
      ],
      "metadata": {
        "id": "XapQ8ny_pIj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6002db-0f5b-46f5-d9e2-84eb12025596"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2165: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"saya mau permakanan nasi\"\n",
        "enc = tokenizer.tokenize(sentence)\n",
        "enc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm_0i6YbpQzC",
        "outputId": "c7f39a27-f5f5-454e-9f95-80cfb8ff8b5f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['saya', 'mau', 'perm', '##akan', '##an', 'nasi']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the tokenizer\n",
        "\n",
        "import pickle\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "Mvh8Rpyp-ZWt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load tokenizer\n",
        "\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "metadata": {
        "id": "RkvO-lYY-ZZG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= LineByLineTextDataset(\n",
        "    tokenizer = tokenizer,\n",
        "    file_path = '/content/drive/MyDrive/Bootcamp AI/Dataset/clean_id.txt',\n",
        "    block_size = 128  # maximum sequence length\n",
        ")\n",
        "\n",
        "print('No. of lines: ', len(dataset)) # No of lines in your datset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ufGg6TOpTa9",
        "outputId": "65d1d53f-0e85-4cfd-80d6-f78cb347ccf0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of lines:  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Model"
      ],
      "metadata": {
        "id": "uD4Dsu99NWSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n",
        "\n",
        "config = BertConfig(\n",
        "    vocab_size=50000,\n",
        "    hidden_size=768,\n",
        "    num_hidden_layers=6,\n",
        "    num_attention_heads=12,\n",
        "    max_position_embeddings=512\n",
        ")\n",
        "\n",
        "model = BertForMaskedLM(config)\n",
        "print('No of parameters: ', model.num_parameters())\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "metadata": {
        "id": "HHlMtNR3pcd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed43668b-2b92-4594-8a00-f4c1eac1ea3c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of parameters:  81965648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Training Arguments"
      ],
      "metadata": {
        "id": "Mo2NaKRUNpq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "l7DW91LDqASb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./models',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=64,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        ")"
      ],
      "metadata": {
        "id": "8YOumbPaqLRB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")"
      ],
      "metadata": {
        "id": "6_o37r_0qN7N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "trainer.train()\n",
        "trainer.save_model('./models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "QE8Qa26xqPvQ",
        "outputId": "7fb4c157-22eb-49c6-d38c-f2fd9ae11806"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='628' max='628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [628/628 05:48, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>7.746600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5min 9s, sys: 3.2 s, total: 5min 12s\n",
            "Wall time: 5min 58s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch torchvision --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "g-9LKzo4-Q_B",
        "outputId": "469548fe-8c97-49ef-c120-c555e1663c44"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "Collecting torch\n",
            "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Collecting triton==3.0.0 (from torch)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.18.1\n",
            "    Uninstalling nvidia-nccl-cu12-2.18.1:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.18.1\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
            "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0\n",
            "    Uninstalling torch-2.1.0:\n",
            "      Successfully uninstalled torch-2.1.0\n",
            "Successfully installed nvidia-cudnn-cu12-9.1.0.70 nvidia-nccl-cu12-2.20.5 torch-2.4.1 triton-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "d45af39cf4114d388545e974fcf8a0e1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "5lWOr-4QO6IB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForMaskedLM.from_pretrained('./models')\n"
      ],
      "metadata": {
        "id": "SybgFJtSPA6b"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "ORjVg2vYPHU3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "121342d5-431d-4cd4-dd93-70a00db2472d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(\"apa [MASK] kamu mau?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCqG085ePKzG",
        "outputId": "cc61aceb-563a-4659-af85-3b8c19773e90"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.03366991505026817,\n",
              "  'token': 15,\n",
              "  'token_str': ',',\n",
              "  'sequence': 'apa, kamu mau?'},\n",
              " {'score': 0.02828366681933403,\n",
              "  'token': 202,\n",
              "  'token_str': 'itu',\n",
              "  'sequence': 'apa itu kamu mau?'},\n",
              " {'score': 0.021805882453918457,\n",
              "  'token': 169,\n",
              "  'token_str': 'yang',\n",
              "  'sequence': 'apa yang kamu mau?'},\n",
              " {'score': 0.016806859523057938,\n",
              "  'token': 222,\n",
              "  'token_str': 'ini',\n",
              "  'sequence': 'apa ini kamu mau?'},\n",
              " {'score': 0.01202622801065445,\n",
              "  'token': 16,\n",
              "  'token_str': '-',\n",
              "  'sequence': 'apa - kamu mau?'}]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(\"apa [MASK] mereka berdua?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuS04YG9PdcJ",
        "outputId": "191082c8-1aba-4291-aca4-e9395830924e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.03421563655138016,\n",
              "  'token': 15,\n",
              "  'token_str': ',',\n",
              "  'sequence': 'apa, mereka berdua?'},\n",
              " {'score': 0.027160219848155975,\n",
              "  'token': 202,\n",
              "  'token_str': 'itu',\n",
              "  'sequence': 'apa itu mereka berdua?'},\n",
              " {'score': 0.021099412813782692,\n",
              "  'token': 169,\n",
              "  'token_str': 'yang',\n",
              "  'sequence': 'apa yang mereka berdua?'},\n",
              " {'score': 0.01625778153538704,\n",
              "  'token': 222,\n",
              "  'token_str': 'ini',\n",
              "  'sequence': 'apa ini mereka berdua?'},\n",
              " {'score': 0.012533589266240597,\n",
              "  'token': 16,\n",
              "  'token_str': '-',\n",
              "  'sequence': 'apa - mereka berdua?'}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(\"aku memohon pada [MASK] untuk mengampuniku\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FreoVsHu0STx",
        "outputId": "a8963715-f06a-4e85-ecb5-0104b2f36103"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.5188763737678528,\n",
              "  'token': 17,\n",
              "  'token_str': '.',\n",
              "  'sequence': 'aku memohon pada. untuk mengampuniku'},\n",
              " {'score': 0.02297690510749817,\n",
              "  'token': 32,\n",
              "  'token_str': '?',\n",
              "  'sequence': 'aku memohon pada? untuk mengampuniku'},\n",
              " {'score': 0.009551445953547955,\n",
              "  'token': 15,\n",
              "  'token_str': ',',\n",
              "  'sequence': 'aku memohon pada, untuk mengampuniku'},\n",
              " {'score': 0.006079860497266054,\n",
              "  'token': 222,\n",
              "  'token_str': 'ini',\n",
              "  'sequence': 'aku memohon pada ini untuk mengampuniku'},\n",
              " {'score': 0.006033271551132202,\n",
              "  'token': 224,\n",
              "  'token_str': 'tidak',\n",
              "  'sequence': 'aku memohon pada tidak untuk mengampuniku'}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(\"Arsenal adalah tim [MASK] terkenal Inggris\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un_0eJwI_nJ3",
        "outputId": "562ee57f-8b22-4175-c400-4b3e0d46a8f7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.5727763175964355,\n",
              "  'token': 17,\n",
              "  'token_str': '.',\n",
              "  'sequence': 'arsenal adalah tim. terkenal inggris'},\n",
              " {'score': 0.026539623737335205,\n",
              "  'token': 32,\n",
              "  'token_str': '?',\n",
              "  'sequence': 'arsenal adalah tim? terkenal inggris'},\n",
              " {'score': 0.006426138803362846,\n",
              "  'token': 15,\n",
              "  'token_str': ',',\n",
              "  'sequence': 'arsenal adalah tim, terkenal inggris'},\n",
              " {'score': 0.0046910797245800495,\n",
              "  'token': 169,\n",
              "  'token_str': 'yang',\n",
              "  'sequence': 'arsenal adalah tim yang terkenal inggris'},\n",
              " {'score': 0.0038043742533773184,\n",
              "  'token': 224,\n",
              "  'token_str': 'tidak',\n",
              "  'sequence': 'arsenal adalah tim tidak terkenal inggris'}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! tar -czvf bert_news_id_100k.tar.gz /content/models /content/daily-id-vocab.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyITU9Ly2zH8",
        "outputId": "2eb4f4ad-437b-46c6-9835-d5248030489a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: Removing leading `/' from member names\n",
            "/content/models/\n",
            "/content/models/model.safetensors\n",
            "tar: Removing leading `/' from hard link targets\n",
            "/content/models/training_args.bin\n",
            "/content/models/runs/\n",
            "/content/models/runs/Oct04_14-31-23_627cf0a1985a/\n",
            "/content/models/runs/Oct04_14-31-23_627cf0a1985a/events.out.tfevents.1728052353.627cf0a1985a.2527.0\n",
            "/content/models/checkpoint-628/\n",
            "/content/models/checkpoint-628/model.safetensors\n",
            "/content/models/checkpoint-628/training_args.bin\n",
            "/content/models/checkpoint-628/optimizer.pt\n",
            "/content/models/checkpoint-628/scheduler.pt\n",
            "/content/models/checkpoint-628/rng_state.pth\n",
            "/content/models/checkpoint-628/generation_config.json\n",
            "/content/models/checkpoint-628/config.json\n",
            "/content/models/checkpoint-628/trainer_state.json\n",
            "/content/models/generation_config.json\n",
            "/content/models/config.json\n",
            "/content/daily-id-vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /content/bert_news_id_100k.tar.gz \"/content/bert_news_id_100k.tar.gz\""
      ],
      "metadata": {
        "id": "8XJ0ScCk6R4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3af7823-d321-47df-8bc9-8307b985269b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: '/content/bert_news_id_100k.tar.gz' and '/content/bert_news_id_100k.tar.gz' are the same file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j2fi3KGEAVkr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bAZZloWeVIWk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}